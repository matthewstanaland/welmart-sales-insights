from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, count, year

# âœ… Initialize Spark Session
spark = SparkSession.builder.appName("Welmart Sales Analysis").getOrCreate()

# âœ… Load dataset (Ensure 'Superstore.csv' is in the same folder)
file_path = "Superstore.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)

# âœ… Print Schema (For Debugging)
df.printSchema()

# âœ… Best-selling product sub-category
best_selling_sub_category = (
    df.groupBy("Sub-Category")
    .agg(sum("Quantity").alias("Total_Quantity"))
    .orderBy("Total_Quantity", ascending=False)
    .first()["Sub-Category"]
)

# âœ… Product category generating the highest revenue
highest_revenue_category = (
    df.groupBy("Category")
    .agg(sum("Sales").alias("Total_Sales"))
    .orderBy("Total_Sales", ascending=False)
    .first()["Category"]
)

# âœ… State with highest number of orders
state_with_most_orders = (
    df.groupBy("State")
    .agg(count("*").alias("Order_Count"))
    .orderBy("Order_Count", ascending=False)
    .first()["State"]
)

# âœ… Year with highest revenue
df = df.withColumn("Year", year(df["Order Date"]))
highest_revenue_year = (
    df.groupBy("Year")
    .agg(sum("Sales").alias("Total_Sales"))
    .orderBy("Total_Sales", ascending=False)
    .first()["Year"]
)

# âœ… Top 10 most valuable customers
top_customers = (
    df.groupBy("Customer ID")
    .agg(sum("Sales").alias("Total_Sales"))
    .orderBy("Total_Sales", ascending=False)
    .limit(10)
)

# âœ… Print Results
print("\nğŸ”¹ Best-selling product sub-category:", best_selling_sub_category)
print("ğŸ”¹ Product category generating highest revenue:", highest_revenue_category)
print("ğŸ”¹ State with highest number of orders:", state_with_most_orders)
print("ğŸ”¹ Year with highest revenue:", highest_revenue_year)

print("\nğŸ† Top 10 Most Valuable Customers:")
top_customers.show()

print("\nâœ… Welmart analysis completed successfully!")

# âœ… Stop Spark Session
spark.stop()
